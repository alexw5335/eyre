
Codes            Mnemonic        Explanation

\0                                       terminates the code. (Unless it's a literal of course.)
\1..\4                                   that many literal bytes follow in the code stream
\5                                       add 4 to the primary operand number (b, low octdigit)
\6                                       add 4 to the secondary operand number (a, middle octdigit)
\7                                       add 4 to both the primary and the secondary operand number
\10..\13                                 a literal byte follows in the code stream, to be added
                                         to the register value of operand 0..3
\14..\17                                 the position of index register operand in MIB (BND insns)
\20..\23         ib                      a byte immediate operand, from operand 0..3
\24..\27         ib,u                    a zero-extended byte immediate operand, from operand 0..3
\30..\33         iw                      a word immediate operand, from operand 0..3
\34..\37         iwd                     select between \3[0-3] and \4[0-3] depending on 16/32 bit
                                         assembly mode or the operand-size override on the operand
\40..\43         id                      a long immediate operand, from operand 0..3
\44..\47         iwdq                    select between \3[0-3], \4[0-3] and \5[4-7]
                                         depending on the address size of the instruction.
\50..\53         rel8                    a byte relative operand, from operand 0..3
\54..\57         iq                      a qword immediate operand, from operand 0..3
\60..\63         rel16                   a word relative operand, from operand 0..3
\64..\67         rel                     select between \6[0-3] and \7[0-3] depending on 16/32 bit
                                         assembly mode or the operand-size override on the operand
\70..\73         rel32                   a long relative operand, from operand 0..3
\74..\77         seg                     a word constant, from the _segment_ part of operand 0..3
\1ab             /r                      a ModRM, calculated on EA in operand a, with the reg
                                         field the register value of operand b.
\171\mab         /mrb (e.g /3r0)         a ModRM, with the reg field taken from operand a, and the m
                                         and b fields set to the specified values.
\172\ab          /is4                    the register number from operand a in bits 7..4, with
                                         the 4-bit immediate from operand b in bits 3..0.
\173\xab                                 the register number from operand a in bits 7..4, with
                                         the value b in bits 3..0.
\174..\177                               the register number from operand 0..3 in bits 7..4, and
                                         an arbitrary value in bits 3..0 (assembled as zero.)
\2ab             /b                      a ModRM, calculated on EA in operand a, with the reg
                                         field equal to digit b.
\240..\243                               this instruction uses EVEX rather than REX or VEX/XOP, with the
                                         V field taken from operand 0..3.
\250                                     this instruction uses EVEX rather than REX or VEX/XOP, with the
                                         V field set to 1111b.


EVEX prefixes are followed by the sequence:
\cm\wlp\tup    where cm is:
                 cc 00m mmm
                 c = 2 for EVEX and mmmm is the M field (EVEX.P0[3:0])
               and wlp is:
                 00 wwl lpp
                 [l0]  ll = 0 (.128, .lz)
                 [l1]  ll = 1 (.256)
                 [l2]  ll = 2 (.512)
                 [lig] ll = 3 for EVEX.L'L don't care (always assembled as 0)

                 [w0]  ww = 0 for W = 0
                 [w1]  ww = 1 for W = 1
                 [wig] ww = 2 for W don't care (always assembled as 0)
                 [ww]  ww = 3 for W used as REX.W

                 [p0]  pp = 0 for no prefix
                 [60]  pp = 1 for legacy prefix 60
                 [f3]  pp = 2
                 [f2]  pp = 3

               tup is tuple type for Disp8*N from %tuple_codes in insns.pl
                   (compressed displacement encoding)

\254..\257       id,s                    a signed 32-bit operand to be extended to 64 bits.
\260..\263                               this instruction uses VEX/XOP rather than REX, with the
                                         V field taken from operand 0..3.
\270                                     this instruction uses VEX/XOP rather than REX, with the
                                         V field set to 1111b.
VEX/XOP prefixes are followed by the sequence:
\tmm\wlp        where mm is the M field; and wlp is:
                00 wwl lpp
                [l0]  ll = 0 for L = 0 (.128, .lz)
                [l1]  ll = 1 for L = 1 (.256)
                [lig] ll = 2 for L don't care (always assembled as 0)

                [w0]  ww = 0 for W = 0
                [w1 ] ww = 1 for W = 1
                [wig] ww = 2 for W don't care (always assembled as 0)
                [ww]  ww = 3 for W used as REX.W

t = 0 for VEX (C4/C5), t = 1 for XOP (8F).

\271             hlexr                       instruction takes XRELEASE (F3) with or without lock
\272             hlenl                       instruction takes XACQUIRE/XRELEASE with or without lock
\273             hle                         instruction takes XACQUIRE/XRELEASE with lock only
\274..\277       ib,s                        a byte immediate operand, from operand 0..3, sign-extended
                                             to the operand size (if o16/o32/o64 present) or the bit size
\310             a16                         indicates fixed 16-bit address size, i.e. optional 0x67.
\311             a32                         indicates fixed 32-bit address size, i.e. optional 0x67.
\312             adf                         (disassembler only) invalid with non-default address size.
\313             a64                         indicates fixed 64-bit address size, 0x67 invalid.
\314             norexb                      (disassembler only) invalid with REX.B
\315             norexx                      (disassembler only) invalid with REX.X
\316             norexr                      (disassembler only) invalid with REX.R
\317             norexw                      (disassembler only) invalid with REX.W
\320             o16                         indicates fixed 16-bit operand size, i.e. optional 0x66.
\321             o32                         indicates fixed 32-bit operand size, i.e. optional 0x66.
\322             odf                         indicates that this instruction is only valid when the
                                             operand size is the default (instruction to disassembler,
                                             generates no code in the assembler)
\323             o64nw                       indicates fixed 64-bit operand size, REX on extensions only.
\324             o64                         indicates 64-bit operand size requiring REX prefix.
\325             nohi                        instruction which always uses spl/bpl/sil/dil
\326             nof3                        instruction not valid with 0xF3 REP prefix.  Hint for
                                                disassembler only; for SSE instructions.
\331             norep                       instruction not valid with REP prefix.  Hint for
                                             disassembler only; for SSE instructions.
\332             f2i                         REP prefix (0xF2 byte) used as opcode extension.
\333             f3i                         REP prefix (0xF3 byte) used as opcode extension.
\334             rex.l                       LOCK prefix used as REX.R (used in non-64-bit mode)
\335             repe                        disassemble a rep (0xF3 byte) prefix as repe not rep.
\336             mustrep                     force a REP(E) prefix (0xF3) even if not specified.
\337             mustrepne                   force a REPNE prefix (0xF2) even if not specified.
                                             \336-\337 are still listed as prefixes in the disassembler.
\340             resb                        reserve <operand 0> bytes of uninitialized storage.
                                             Operand 0 had better be a segmentless constant.
\341             wait                        this instruction needs a WAIT "prefix"
\360             np                          no SSE prefix (== \364\331)
\361                                         66 SSE prefix (== \366\331)
\364             !osp                        operand-size prefix (0x66) not permitted
\365             !asp                        address-size prefix (0x67) not permitted
\366                                         operand-size prefix (0x66) used as opcode extension
\367                                         address-size prefix (0x67) used as opcode extension
\370,\371        jcc8                        match only if operand 0 meets byte jump criteria.
                 jmp8                        370 is used for Jcc, 371 is used for JMP.
\373             jlen                        assemble 0x03 if bits==16, 0x05 if bits==32;
                                             used for conditional jump over longer jump
\374             vsibx|vm32x|vm64x           this instruction takes an XMM VSIB memory EA
\375             vsiby|vm32y|vm64y           this instruction takes an YMM VSIB memory EA
\376             vsibz|vm32z|vm64z           this instruction takes an ZMM VSIB memory EA


if_("SM",                "Size match");
if_("SM2",               "Size match first two operands");
if_("SB",                "Unsized operands can't be non-byte");
if_("SW",                "Unsized operands can't be non-word");
if_("SD",                "Unsized operands can't be non-dword");
if_("SQ",                "Unsized operands can't be non-qword");
if_("SO",                "Unsized operands can't be non-oword");
if_("SY",                "Unsized operands can't be non-yword");
if_("SZ",                "Unsized operands can't be non-zword");
if_("SIZE",              "Unsized operands must match the bitsize");
if_("SX",                "Unsized operands not allowed");
if_("ANYSIZE",           "Ignore operand size even if explicit");
if_("AR0",               "SB, SW, SD applies to argument 0");
if_("AR1",               "SB, SW, SD applies to argument 1");
if_("AR2",               "SB, SW, SD applies to argument 2");
if_("AR3",               "SB, SW, SD applies to argument 3");
if_("AR4",               "SB, SW, SD applies to argument 4");
if_("OPT",               "Optimizing assembly only");
if_("LATEVEX",            "Only if EVEX instructions are disabled");

if_("PRIV",              "Privileged instruction");
if_("SMM",               "Only valid in SMM");
if_("PROT",              "Protected mode only");
if_("LOCK",              "Lockable if operand 0 is memory");
if_("NOLONG",            "Not available in long mode");
if_("LONG",              "Long mode");
if_("NOHLE",             "HLE prefixes forbidden");
if_("MIB",               "split base/index EA");      12, Only used with some MMX and AMXTILE instructions
if_("SIB",               "SIB encoding required");    3, Only used with AMXTILE instructions
if_("BND",               "BND (0xF2) prefix available");
if_("UNDOC",             "Undocumented");
if_("HLE",               "HLE prefixed");
if_("FPU",               "FPU");
if_("MMX",               "MMX");
if_("3DNOW",             "3DNow!");
if_("SSE",               "SSE (KNI, MMX2)");
if_("SSE2",              "SSE2");
if_("SSE3",              "SSE3 (PNI)");
if_("VMX",               "VMX");
if_("SSSE3",             "SSSE3");
if_("SSE4A",             "AMD SSE4a");
if_("SSE41",             "SSE4.1");
if_("SSE42",             "SSE4.2");
if_("SSE5",              "SSE5");
if_("AVX",               "AVX  (256-bit floating point)");
if_("AVX2",              "AVX2 (256-bit integer)");
if_("FMA",               "");
if_("BMI1",              "");
if_("BMI2",              "");
if_("TBM",               "");
if_("RTM",               "");
if_("INVPCID",           "");
if_("AVX512",            "AVX-512F (512-bit base architecture)");
if_("AVX512CD",          "AVX-512 Conflict Detection");
if_("AVX512ER",          "AVX-512 Exponential and Reciprocal");
if_("AVX512PF",          "AVX-512 Prefetch");
if_("MPX",               "MPX");
if_("SHA",               "SHA");
if_("PREFETCHWT1",       "PREFETCHWT1");
if_("AVX512VL",          "AVX-512 Vector Length Orthogonality");
if_("AVX512DQ",          "AVX-512 Dword and Qword");
if_("AVX512BW",          "AVX-512 Byte and Word");
if_("AVX512IFMA",        "AVX-512 IFMA instructions");
if_("AVX512VBMI",        "AVX-512 VBMI instructions");
if_("AES",               "AES instructions");
if_("VAES",              "AES AVX instructions");
if_("VPCLMULQDQ",        "AVX Carryless Multiplication");
if_("GFNI",              "Galois Field instructions");
if_("AVX512VBMI2",       "AVX-512 VBMI2 instructions");
if_("AVX512VNNI",        "AVX-512 VNNI instructions");
if_("AVX512BITALG",      "AVX-512 Bit Algorithm instructions");
if_("AVX512VPOPCNTDQ",   "AVX-512 VPOPCNTD/VPOPCNTQ");
if_("AVX5124FMAPS",      "AVX-512 4-iteration multiply-add");
if_("AVX5124VNNIW",      "AVX-512 4-iteration dot product");
if_("AVX512FP16",        "AVX-512 FP16 instructions");
if_("AVX512FC16",        "AVX-512 FC16 instructions");
if_("SGX",               "Intel Software Guard Extensions (SGX)");
if_("CET",               "Intel Control-Flow Enforcement Technology (CET)");
if_("ENQCMD",            "Enqueue command instructions");
if_("PCONFIG",           "Platform configuration instruction");
if_("WBNOINVD",          "Writeback and do not invalidate instruction");
if_("TSXLDTRK",          "TSX suspend load address tracking");
if_("SERIALIZE",         "SERIALIZE instruction");
if_("AVX512BF16",        "AVX-512 bfloat16");
if_("AVX512VP2INTERSECT", "AVX-512 VP2INTERSECT instructions");
if_("AMXTILE",           "AMX tile configuration instructions");
if_("AMXBF16",           "AMX bfloat16 multiplication");
if_("AMXINT8",           "AMX 8-bit integer multiplication");
if_("FRED",              "Flexible Return and Exception Delivery (FRED)");
if_("RAOINT",		 "Remote atomic operations (RAO-INT)");
if_("UINTR",		 "User interrupts");
if_("CMPCCXADD",         "CMPccXADD instructions");
if_("PREFETCHI",         "PREFETCHI0 and PREFETCHI1");
if_("WRMSRNS",		 "WRMSRNS");
if_("MSRLIST",           "RDMSRLIST and WRMSRLIST");
if_("AVXNECONVERT",	 "AVX exceptionless floating-point conversions");
if_("AVXVNNIINT8",       "AVX Vector Neural Network 8-bit integer instructions");
if_("AVXIFMA",           "AVX integer multiply and add");
if_("HRESET",            "History reset");

if_("OBSOLETE",          "Instruction removed from architecture");
if_("NEVER",             "Instruction never implemented");
if_("NOP",               "Instruction is always a (nonintentional) NOP");
if_("VEX",               "VEX or XOP encoded instruction");
if_("EVEX",              "EVEX encoded instruction");

if_("8086",              "8086");
if_("186",               "186+");
if_("286",               "286+");
if_("386",               "386+");
if_("486",               "486+");
if_("PENT",              "Pentium");
if_("P6",                "P6");
if_("KATMAI",            "Katmai");
if_("WILLAMETTE",        "Willamette");
if_("PRESCOTT",          "Prescott");
if_("X86_64",            "x86-64 (long or legacy mode)");
if_("NEHALEM",           "Nehalem");
if_("WESTMERE",          "Westmere");
if_("SANDYBRIDGE",       "Sandy Bridge");
if_("FUTURE",            "Ivy Bridge or newer");
if_("IA64",              "IA64 (in x86 mode)");

# Default CPU level
if_("DEFAULT",           "Default CPU level");

# Must be the last CPU definition
if_("ANY",               "Allow any known instruction");

# These must come after the CPU definitions proper
if_("CYRIX",             "Cyrix-specific");
if_("AMD",               "AMD-specific");



- Operands starting with 'sbyte' can be ignored. They are imm16/32/64 that can fit into smaller widths.
- OPT is only used 6 times, and only for alternative operands, can be omitted
- ND seems to only be used for alternative forms? Used many times, probably can't be omitted for most cases
	- Used for FADD/FMUL void forms
	- Used for imm variances (sbyte, etc.)
	- Can be ignored when paired with certain operands
	- 'Not Documented'? Used for encodings that don't exactly match the Intel Manual
	- Used for all KMOV instructions (Intel Manual uses KMOVB, KMOVD, etc., NASM only uses KMOV)
	- Seems to be used for alias instructions (can be omitted)
- VEX, EVEX, DEFAULT, ANY, NOP: Never used
- HLE: Used once (XTEST)
- NOHLE: Only used with MOV MOFFS (4 times)
- SIB: Only used with AMXTILE (3 times)
- MIB: Used 12 times (AMXTILE, MPX)
- iwdq: Only used with MOV moffs
- AR3, AR4: Never used
- AR0: Only used with PUSH (8 times)
- AR1: Used 30 times
- AR2: Used 35 times
- ANYSIZE: Only used with LEA (6 times)
- SIZE: Only used with PUSH IMM (8 times)
- LATEVEX: Used 30 times. Obsolete instructions? They don't appear in the Intel manual, can be omitted?
- vm32x/vm32y: Only used with VGATHER... (8 times)
- vm64x/vm64y: Only used VGATHER... (8 times)
- vm32z/vm64z: Never used
- vsibx/vsiby/vsibz: Used 64 times
- wait: Only used with FPU instructions (10 times), insert 9B before opcode
- hlexr: Only used with MOV (12 times)
- nohi: Only used twice, can be omitted
- SM2: Only used with encodings with 3 operands (IMUL, SHLD, SHRD, PSHUFxx, 20 times)
- SM: Only used with encoding with 2 operands, used many times
- SX: Only used with AMXTILE (3 times, excluding ND and LATEVEX instructions)
- Unsized memory operands must not take a width (e.g. SGDT)

- ERROR? PMULUDQ and PSUBQ with MMX registers should be SQ not SO

Multiple Extras:
SM2_SB_AR2  (12, SHLD, SHRD)
SD_AR1  (10, CVTSI2SS/CVTSS2SI/CVTTSS2SI)
SQ_AR1  (10)
SB_AR2  (17)
SB_AR1  (10)
SM2_SB_AR2 (4)
SB_SM  (1, KMOV)

SHLD mem,reg16,imm [mri:, o16, 0f, a4, /r, ib,u] [386, SM2, SB, AR2]
SHLD reg16,reg16,imm [mri:, o16, 0f, a4, /r, ib,u] [386, SM2, SB, AR2]
SHLD mem,reg32,imm [mri:, o32, 0f, a4, /r, ib,u] [386, SM2, SB, AR2]
SHLD reg32,reg32,imm [mri:, o32, 0f, a4, /r, ib,u] [386, SM2, SB, AR2]
SHLD mem,reg64,imm [mri:, o64, 0f, a4, /r, ib,u] [X86_64, LONG, SM2, SB, AR2]
SHLD reg64,reg64,imm [mri:, o64, 0f, a4, /r, ib,u] [X86_64, LONG, SM2, SB, AR2]
SHRD mem,reg16,imm [mri:, o16, 0f, ac, /r, ib,u] [386, SM2, SB, AR2]
SHRD reg16,reg16,imm [mri:, o16, 0f, ac, /r, ib,u] [386, SM2, SB, AR2]
SHRD mem,reg32,imm [mri:, o32, 0f, ac, /r, ib,u] [386, SM2, SB, AR2]
SHRD reg32,reg32,imm [mri:, o32, 0f, ac, /r, ib,u] [386, SM2, SB, AR2]
SHRD mem,reg64,imm [mri:, o64, 0f, ac, /r, ib,u] [X86_64, LONG, SM2, SB, AR2]
SHRD reg64,reg64,imm [mri:, o64, 0f, ac, /r, ib,u] [X86_64, LONG, SM2, SB, AR2]

CVTSI2SS xmmreg,mem [rm:, f3, 0f, 2a, /r] [KATMAI, SSE, SD, AR1, ND]
CVTSI2SS xmmreg,rm32 [rm:, f3, 0f, 2a, /r] [KATMAI, SSE, SD, AR1]

CVTSI2SS xmmreg,rm64 [rm:, o64, f3, 0f, 2a, /r] [X86_64, LONG, SSE, SQ, AR1]

CVTSS2SI reg32,xmmreg [rm:, f3, 0f, 2d, /r] [KATMAI, SSE, SD, AR1]
CVTSS2SI reg32,mem [rm:, f3, 0f, 2d, /r] [KATMAI, SSE, SD, AR1]
CVTSS2SI reg64,xmmreg [rm:, o64, f3, 0f, 2d, /r] [X86_64, LONG, SSE, SD, AR1]
CVTSS2SI reg64,mem [rm:, o64, f3, 0f, 2d, /r] [X86_64, LONG, SSE, SD, AR1]
CVTTSS2SI reg32,xmmrm [rm:, f3, 0f, 2c, /r] [KATMAI, SSE, SD, AR1]
CVTTSS2SI reg64,xmmrm [rm:, o64, f3, 0f, 2c, /r] [X86_64, LONG, SSE, SD, AR1]

PEXTRW reg32,mmxreg,imm [rmi:, np, 0f, c5, /r, ib,u] [KATMAI, MMX, SB, AR2]
PINSRW mmxreg,mem,imm [rmi:, np, 0f, c4, /r, ib,u] [KATMAI, MMX, SB, AR2]
PINSRW mmxreg,rm16,imm [rmi:, np, 0f, c4, /r, ib,u] [KATMAI, MMX, SB, AR2]
PINSRW mmxreg,reg32,imm [rmi:, np, 0f, c4, /r, ib,u] [KATMAI, MMX, SB, AR2]

PSHUFW mmxreg,mmxrm,imm [rmi:, np, o64nw, 0f, 70, /r, ib] [KATMAI, MMX, SM2, SB, AR2]

PEXTRW reg32,xmmreg,imm [rmi:, 66, 0f, c5, /r, ib,u] [WILLAMETTE, SSE2, SB, AR2]
PEXTRW reg64,xmmreg,imm [rmi:, 66, 0f, c5, /r, ib,u] [X86_64, LONG, SSE2, SB, AR2, ND]
PINSRW xmmreg,reg16,imm [rmi:, 66, 0f, c4, /r, ib,u] [WILLAMETTE, SSE2, SB, AR2]
PINSRW xmmreg,reg32,imm [rmi:, 66, 0f, c4, /r, ib,u] [WILLAMETTE, SSE2, SB, AR2, ND]
PINSRW xmmreg,reg64,imm [rmi:, 66, 0f, c4, /r, ib,u] [X86_64, LONG, SSE2, SB, AR2, ND]
PINSRW xmmreg,mem,imm [rmi:, 66, 0f, c4, /r, ib,u] [WILLAMETTE, SSE2, SB, AR2]
PINSRW xmmreg,mem16,imm [rmi:, 66, 0f, c4, /r, ib,u] [WILLAMETTE, SSE2, SB, AR2]
PSHUFD xmmreg,xmmreg,imm [rmi:, 66, 0f, 70, /r, ib] [WILLAMETTE, SSE2, SB, AR2]

PSHUFD xmmreg,mem,imm [rmi:, 66, 0f, 70, /r, ib] [WILLAMETTE, SSE2, SM2, SB, AR2]
PSHUFHW xmmreg,xmmreg,imm [rmi:, f3, 0f, 70, /r, ib] [WILLAMETTE, SSE2, SB, AR2]
PSHUFHW xmmreg,mem,imm [rmi:, f3, 0f, 70, /r, ib] [WILLAMETTE, SSE2, SM2, SB, AR2]
PSHUFLW xmmreg,xmmreg,imm [rmi:, f2, 0f, 70, /r, ib] [WILLAMETTE, SSE2, SB, AR2]
PSHUFLW xmmreg,mem,imm [rmi:, f2, 0f, 70, /r, ib] [WILLAMETTE, SSE2, SM2, SB, AR2]

PSLLDQ xmmreg,imm [mi:, 66, 0f, 73, /7, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSLLW xmmreg,imm [mi:, 66, 0f, 71, /6, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSLLD xmmreg,imm [mi:, 66, 0f, 72, /6, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSLLQ xmmreg,imm [mi:, 66, 0f, 73, /6, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSRAW xmmreg,imm [mi:, 66, 0f, 71, /4, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSRAD xmmreg,imm [mi:, 66, 0f, 72, /4, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSRLDQ xmmreg,imm [mi:, 66, 0f, 73, /3, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSRLW xmmreg,imm [mi:, 66, 0f, 71, /2, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSRLD xmmreg,imm [mi:, 66, 0f, 72, /2, ib,u] [WILLAMETTE, SSE2, SB, AR1]
PSRLQ xmmreg,imm [mi:, 66, 0f, 73, /2, ib,u] [WILLAMETTE, SSE2, SB, AR1]

CVTSD2SI reg32,xmmreg [rm:, norexw, f2, 0f, 2d, /r] [WILLAMETTE, SSE2, SQ, AR1]
CVTSD2SI reg32,mem [rm:, norexw, f2, 0f, 2d, /r] [WILLAMETTE, SSE2, SQ, AR1]
CVTSD2SI reg64,xmmreg [rm:, o64, f2, 0f, 2d, /r] [X86_64, LONG, SSE2, SQ, AR1]
CVTSD2SI reg64,mem [rm:, o64, f2, 0f, 2d, /r] [X86_64, LONG, SSE2, SQ, AR1]

CVTSI2SD xmmreg,mem [rm:, f2, 0f, 2a, /r] [WILLAMETTE, SSE2, SD, AR1, ND]
CVTSI2SD xmmreg,rm32 [rm:, norexw, f2, 0f, 2a, /r] [WILLAMETTE, SSE2, SD, AR1]

CVTSI2SD xmmreg,rm64 [rm:, o64, f2, 0f, 2a, /r] [X86_64, LONG, SSE2, SQ, AR1]
CVTTSD2SI reg32,xmmreg [rm:, norexw, f2, 0f, 2c, /r] [WILLAMETTE, SSE2, SQ, AR1]
CVTTSD2SI reg32,mem [rm:, norexw, f2, 0f, 2c, /r] [WILLAMETTE, SSE2, SQ, AR1]
CVTTSD2SI reg64,xmmreg [rm:, o64, f2, 0f, 2c, /r] [X86_64, LONG, SSE2, SQ, AR1]
CVTTSD2SI reg64,mem [rm:, o64, f2, 0f, 2c, /r] [X86_64, LONG, SSE2, SQ, AR1]

PINSRB xmmreg,mem,imm8 [rmi:, 66, 0f, 3a, 20, /r, ib,u] [SSE41, SB, AR2]
PINSRB xmmreg,rm8,imm8 [rmi:, nohi, 66, 0f, 3a, 20, /r, ib,u] [SSE41, SB, AR2]
PINSRB xmmreg,reg32,imm8 [rmi:, 66, 0f, 3a, 20, /r, ib,u] [SSE41, SB, AR2]
PINSRD xmmreg,rm32,imm8 [rmi:, norexw, 66, 0f, 3a, 22, /r, ib,u] [SSE41, SB, AR2]
PINSRQ xmmreg,rm64,imm8 [rmi:, o64, 66, 0f, 3a, 22, /r, ib,u] [SSE41, X86_64, LONG, SB, AR2]

KMOV mem8,kreg8 [mr:, vex.l0.66.0f.w0, 91, /r] [FUTURE, ND, SB, SM]



mem
reg8
reg16
reg32
reg64
rm16
imm8
rm32
rm64
reg_al
imm
reg_ax
sbyteword
reg_eax
sbytedword
reg_rax
rm8
sbyteword16
imm16
sbytedword32
imm32
void
imm|near
imm64
imm64|near
mem|far
mem16|far
mem32|far
mem64|far
mem|near
rm64|near
mem64
mem128
mem32
fpureg|to
fpureg
fpu0
mem80
mem16
reg_dx
reg_ecx
imm|short
reg_rcx
reg_edx
reg_sreg
mem_offs
reg_creg
reg_dreg
udword
sdword
mmxreg
mmxrm
reg_fs
reg_gs
sbytedword64
unity
reg_cl
reg32na
xmmreg
xmmrm128
xmmrm32
mmxrm64
xmmrm64
xmmrm
mem8
xmm0
xmmrm16
xmmreg*
ymmreg
ymmreg*
ymmrm256
zmmreg
zmmreg*
zmmrm512
mem256
xmmrm128*
ymmrm256*
xmmrm64*
xmmrm32*
xmem64
ymem64
xmem32
ymem32
bndreg
kreg
krm8
krm32
krm64
krm16
kreg8
kreg32
kreg64
kreg16
xmmreg|mask|z
xmmrm128|b64
ymmreg|mask|z
ymmrm256|b64
zmmreg|mask|z
zmmrm512|b64|er
xmmrm128|b32
ymmrm256|b32
zmmrm512|b32|er
xmmrm64|er
xmmrm32|er
zmmrm512|b32
zmmrm512|b64
kreg|mask
zmmrm512|b64|sae
zmmrm512|b32|sae
xmmrm64|sae
xmmrm32|sae
mem128|mask
mem256|mask
mem512|mask
xmmrm64|b32
ymmrm256|b32|er
ymmrm256|sae
ymmrm256|b32|sae
zmmreg|sae
mem64|mask
xmmreg|er
mem512
xmmreg|mask
ymmreg|mask
zmmreg|mask
zmem32
ymem64|mask
zmem32|mask
zmem64|mask
zmem64
xmmrm128|mask|z
ymmrm256|mask|z
zmmrm512|mask|z
mem32|mask
xmmrm8
mem16|mask
xmmrm128|b32*
ymmrm256|b32*
zmmrm512|b32*
xmmrm128|b64*
ymmrm256|b64*
zmmrm512|b64*
xmem32|mask
ymem32|mask
xmem64|mask
zmmrm512*
zmmreg|rs4
ymmrm128|b32
zmmrm128|b32
kreg|rs2
tmmreg
xmmrm16|b16
ymmrm16|b16
zmmrm16|b16|er
xmmrm16|er
zmmrm16|b16|sae
xmmrm16|sae
xmmrm64|b16
xmmrm128|b16
ymmrm256|b16|er
xmmrm32|b16
xmmrm128|b16|sae
ymmrm256|b16|sae
xmmrm128|b16|er
ymmrm256|b16
zmmrm512|b16|er
rm32|er
rm64|er
zmmrm512|b16|sae
zmmrm512|b16
xmmrm256|b16





mem,reg8
reg8,reg8
mem,reg16
reg16,reg16
mem,reg32
reg32,reg32
mem,reg64
reg64,reg64
reg8,mem
reg16,mem
reg32,mem
reg64,mem
rm16,imm8
rm32,imm8
rm64,imm8
reg_al,imm
reg_ax,sbyteword
reg_ax,imm
reg_eax,sbytedword
reg_eax,imm
reg_rax,sbytedword
reg_rax,imm
rm8,imm
rm16,sbyteword
rm16,imm
rm32,sbytedword
rm32,imm
rm64,sbytedword
rm64,imm
mem,imm8
mem,sbyteword16
mem,imm16
mem,sbytedword32
mem,imm32
void
reg32
reg64
imm
imm|near
imm64
imm64|near
mem|far
mem16|far
mem32|far
mem64|far
mem|near
rm64|near
mem
rm64
mem64
mem128
rm8
rm16
rm32
imm,imm
mem32
fpureg|to
fpureg
fpureg,fpu0
fpu0,fpureg
mem80
mem16
reg_ax
reg16,mem,imm8
reg16,mem,sbyteword
reg16,mem,imm16
reg16,mem,imm
reg16,reg16,imm8
reg16,reg16,sbyteword
reg16,reg16,imm16
reg16,reg16,imm
reg32,mem,imm8
reg32,mem,sbytedword
reg32,mem,imm32
reg32,mem,imm
reg32,reg32,imm8
reg32,reg32,sbytedword
reg32,reg32,imm32
reg32,reg32,imm
reg64,mem,imm8
reg64,mem,sbytedword
reg64,mem,imm32
reg64,mem,imm
reg64,reg64,imm8
reg64,reg64,sbytedword
reg64,reg64,imm32
reg64,reg64,imm
reg16,imm8
reg16,sbyteword
reg16,imm16
reg16,imm
reg32,imm8
reg32,sbytedword
reg32,imm32
reg32,imm
reg64,imm8
reg64,sbytedword
reg64,imm32
reg64,imm
reg_al,reg_dx
reg_ax,reg_dx
reg_eax,reg_dx
reg64,mem128
reg_eax,reg_ecx
reg_rax,reg_ecx
imm|short
imm16
imm32
reg16,reg32
reg16,reg64
reg32,reg16
reg32,reg64
reg64,reg16
reg64,reg32
reg16
imm,reg_ecx
imm,reg_rcx
reg_rax,reg_ecx,reg_edx
reg_eax,reg_ecx,reg_edx
reg_ax,reg_ecx,reg_edx
mem,reg_sreg
reg16,reg_sreg
reg32,reg_sreg
reg64,reg_sreg
rm64,reg_sreg
reg_sreg,mem
reg_sreg,reg16
reg_sreg,reg32
reg_sreg,reg64
reg_sreg,rm64
reg_al,mem_offs
reg_ax,mem_offs
reg_eax,mem_offs
reg_rax,mem_offs
mem_offs,reg_al
mem_offs,reg_ax
mem_offs,reg_eax
mem_offs,reg_rax
reg64,reg_creg
reg_creg,reg64
reg64,reg_dreg
reg_dreg,reg64
reg8,imm
reg64,udword
reg64,sdword
rm64,imm32
mmxreg,rm32
rm32,mmxreg
mmxreg,rm64
rm64,mmxreg
mmxreg,mmxrm
mmxrm,mmxreg
reg16,reg8
reg32,rm8
reg32,rm16
reg64,rm8
reg64,rm16
reg64,rm32
imm,reg_al
imm,reg_ax
imm,reg_eax
reg_dx,reg_al
reg_dx,reg_ax
reg_dx,reg_eax
mmxreg,mem
reg_fs
reg_gs
mmxreg,imm
imm8
sbyteword16
sbytedword64
sbytedword32
rm8,unity
rm8,reg_cl
rm8,imm8
rm16,unity
rm16,reg_cl
rm32,unity
rm32,reg_cl
rm64,unity
rm64,reg_cl
reg_sreg,mem80
mem,reg16,imm
mem,reg32,imm
mem,reg64,imm
mem,reg16,reg_cl
reg16,reg16,reg_cl
mem,reg32,reg_cl
reg32,reg32,reg_cl
mem,reg64,reg_cl
reg64,reg64,reg_cl
mem80,reg_sreg
reg16,rm16
reg32,rm32
reg64,rm64
reg_ax,reg16
reg_eax,reg32na
reg_rax,reg64
reg16,reg_ax
reg32na,reg_eax
reg64,reg_rax
reg8
xmmreg,xmmrm128
xmmreg,xmmrm32
xmmreg,xmmrm128,imm8
xmmreg,xmmrm32,imm8
xmmreg,mmxrm64
mmxreg,xmmrm64
xmmreg,mem
xmmreg,rm32
xmmreg,rm64
reg32,xmmreg
reg64,xmmreg
mmxreg,xmmrm
reg32,xmmrm
reg64,xmmrm
xmmrm128,xmmreg
xmmreg,mem64
mem64,xmmreg
xmmreg,xmmreg
mem128,xmmreg
xmmrm32,xmmreg
mem8
mmxreg,mmxreg
mem,mmxreg
reg32,mmxreg,imm
mmxreg,mem,imm
mmxreg,rm16,imm
mmxreg,reg32,imm
reg32,mmxreg
mmxreg,mmxrm,imm
mem,xmmreg
rm32,xmmreg
mmxreg,xmmreg
rm64,xmmreg
xmmreg,mmxreg
xmmreg,xmmrm
reg32,xmmreg,imm
reg64,xmmreg,imm
xmmreg,reg16,imm
xmmreg,reg32,imm
xmmreg,reg64,imm
xmmreg,mem,imm
xmmreg,mem16,imm
xmmreg,xmmreg,imm
xmmreg,imm
xmmreg,xmmrm64
xmmreg,mmxrm
xmmrm64,xmmreg
xmmreg,mem128
rm64,reg64
xmmreg,xmmrm,imm
xmmreg,imm,imm
xmmreg,xmmreg,imm,imm
mem32,xmmreg
xmmreg,xmmrm128,xmm0
rm32,xmmreg,imm8
reg64,xmmreg,imm8
xmmreg,xmmrm,xmm0
reg32,xmmreg,imm8
mem8,xmmreg,imm8
rm64,xmmreg,imm8
mem16,xmmreg,imm8
xmmreg,mem,imm8
xmmreg,rm8,imm8
xmmreg,reg32,imm8
xmmreg,rm32,imm8
xmmreg,rm64,imm8
xmmreg,xmmrm16
xmmreg,xmmrm64,imm8
reg16,mem16
reg32,mem32
reg64,mem64
mem16,reg16
mem32,reg32
mem64,reg64
xmmreg,xmmreg*,xmmrm128
ymmreg,ymmreg*,ymmrm256
zmmreg,zmmreg*,zmmrm512
xmmreg,xmmreg*,xmmrm64
xmmreg,xmmreg*,xmmrm32
xmmreg,xmmreg*,xmmrm128,imm8
ymmreg,ymmreg*,ymmrm256,imm8
xmmreg,xmmreg*,xmmrm128,xmmreg
ymmreg,ymmreg*,ymmrm256,ymmreg
xmmreg,mem32
ymmreg,mem32
ymmreg,mem64
ymmreg,mem128
xmmreg,xmmreg*,xmmrm64,imm8
ymmreg,xmmrm128
ymmreg,ymmrm256
xmmreg,ymmreg
xmmreg,mem256
reg32,xmmrm64
reg64,xmmrm64
xmmreg,xmmreg*,rm32
xmmreg,xmmreg*,mem32
xmmreg,xmmreg*,rm64
reg32,xmmrm32
reg64,xmmrm32
xmmrm128,ymmreg,imm8
ymmreg,ymmreg*,xmmrm128,imm8
xmmreg,xmmreg*,xmmrm32,imm8
ymmreg,mem256
xmmreg,xmmreg,mem128
ymmreg,ymmreg,mem256
mem128,xmmreg,xmmreg
mem256,ymmreg,ymmreg
ymmrm256,ymmreg
xmmreg,xmmreg*,xmmreg
xmmreg,xmmreg*,mem64
reg64,ymmreg
reg32,ymmreg
mem256,ymmreg
ymmreg,ymmrm256,imm8
xmmreg,xmmreg*,mem8,imm8
xmmreg,xmmreg*,rm8,imm8
xmmreg,xmmreg*,reg32,imm8
xmmreg,xmmreg*,mem16,imm8
xmmreg,xmmreg*,rm16,imm8
xmmreg,xmmreg*,mem32,imm8
xmmreg,xmmreg*,rm32,imm8
xmmreg,xmmreg*,mem64,imm8
xmmreg,xmmreg*,rm64,imm8
xmmreg,xmmreg*,imm8
zmmreg,zmmreg*,zmmrm512,imm8
xmmreg,xmmreg,xmmrm128
ymmreg,ymmreg,ymmrm256
xmmreg,xmmreg,xmmrm32
xmmreg,xmmreg,xmmrm64
xmmrm64,xmmreg,imm8
reg32,rm32,imm32
reg64,rm32,imm32
xmmreg,xmmreg*,xmmreg,xmmrm128
ymmreg,ymmreg*,ymmreg,ymmrm256
xmmreg,xmmreg*,xmmrm64,xmmreg
xmmreg,xmmreg*,xmmreg,xmmrm64
xmmreg,xmmreg*,xmmrm32,xmmreg
xmmreg,xmmreg*,xmmreg,xmmrm32
xmmreg,xmmrm128*
ymmreg,ymmrm256*
xmmreg,xmmrm64*
xmmreg,xmmrm32*
xmmreg,xmmrm128*,xmmreg
xmmreg,xmmrm128*,imm8
ymmreg,xmmreg
ymmreg,ymmreg*,imm8
ymmreg,ymmreg*,xmmrm128
xmmreg,mem8
ymmreg,mem8
xmmreg,mem16
ymmreg,mem16
xmmreg,xmmreg*,mem128
ymmreg,ymmreg*,mem256
mem128,xmmreg*,xmmreg
mem256,ymmreg*,ymmreg
xmmreg,xmem64,xmmreg
ymmreg,xmem64,ymmreg
ymmreg,ymem64,ymmreg
xmmreg,xmem32,xmmreg
ymmreg,ymem32,ymmreg
xmmreg,ymem32,xmmreg
reg32,reg32,rm32
reg64,reg64,rm64
reg32,rm32,reg32
reg64,rm64,reg64
reg64,rm64,imm32
reg32,rm32,imm8
reg64,rm64,imm8
bndreg,mem
bndreg,reg64
bndreg,bndreg
mem,bndreg
bndreg,mem,reg64
mem,reg64,bndreg
mem,bndreg,reg64
kreg,kreg,kreg
kreg,krm8
mem8,kreg
kreg,reg32
kreg,reg8
reg32,kreg
kreg,krm32
mem32,kreg
kreg,krm64
mem64,kreg
kreg,reg64
reg64,kreg
kreg,krm16
mem16,kreg
kreg,reg16
kreg,kreg
kreg,kreg,imm8
kreg8,kreg,kreg
kreg32,kreg,kreg
kreg64,kreg,kreg
kreg16,kreg,kreg
kreg8,krm8
mem8,kreg8
kreg8,reg32
kreg8,reg8
reg32,kreg8
kreg32,krm32
mem32,kreg32
kreg32,reg32
reg32,kreg32
kreg64,krm64
mem64,kreg64
kreg64,reg64
reg64,kreg64
kreg16,krm16
mem16,kreg16
kreg16,reg32
reg32,kreg16
kreg16,reg16
kreg8,kreg8
kreg32,kreg32
kreg64,kreg64
kreg16,kreg16
kreg8,kreg
kreg32,kreg
kreg64,kreg
kreg16,kreg
kreg8,kreg,imm8
kreg32,kreg,imm8
kreg64,kreg,imm8
kreg16,kreg,imm8
kreg16,kreg8,kreg8
kreg64,kreg32,kreg32
kreg32,kreg16,kreg16
xmmreg|mask|z,xmmreg*,xmmrm128|b64
ymmreg|mask|z,ymmreg*,ymmrm256|b64
zmmreg|mask|z,zmmreg*,zmmrm512|b64|er
xmmreg|mask|z,xmmreg*,xmmrm128|b32
ymmreg|mask|z,ymmreg*,ymmrm256|b32
zmmreg|mask|z,zmmreg*,zmmrm512|b32|er
xmmreg|mask|z,xmmreg*,xmmrm64|er
xmmreg|mask|z,xmmreg*,xmmrm32|er
xmmreg|mask|z,xmmreg*,xmmrm128|b32,imm8
ymmreg|mask|z,ymmreg*,ymmrm256|b32,imm8
zmmreg|mask|z,zmmreg*,zmmrm512|b32,imm8
xmmreg|mask|z,xmmreg*,xmmrm128|b64,imm8
ymmreg|mask|z,ymmreg*,ymmrm256|b64,imm8
zmmreg|mask|z,zmmreg*,zmmrm512|b64,imm8
zmmreg|mask|z,zmmreg*,zmmrm512|b64
zmmreg|mask|z,zmmreg*,zmmrm512|b32
xmmreg|mask|z,xmmreg,xmmrm128|b64
ymmreg|mask|z,ymmreg,ymmrm256|b64
zmmreg|mask|z,zmmreg,zmmrm512|b64
xmmreg|mask|z,xmmreg,xmmrm128|b32
ymmreg|mask|z,ymmreg,ymmrm256|b32
zmmreg|mask|z,zmmreg,zmmrm512|b32
ymmreg|mask|z,xmmrm64
zmmreg|mask|z,xmmrm64
ymmreg|mask|z,mem128
zmmreg|mask|z,mem128
zmmreg|mask|z,mem256
xmmreg|mask|z,xmmrm64
ymmreg|mask|z,mem64
zmmreg|mask|z,mem64
ymmreg|mask|z,xmmreg
zmmreg|mask|z,xmmreg
xmmreg|mask|z,mem32
ymmreg|mask|z,mem32
zmmreg|mask|z,mem32
xmmreg|mask|z,xmmreg
kreg|mask,xmmreg,xmmrm128|b64
kreg|mask,ymmreg,ymmrm256|b64
kreg|mask,zmmreg,zmmrm512|b64|sae
kreg|mask,xmmreg,xmmrm128|b32
kreg|mask,ymmreg,ymmrm256|b32
kreg|mask,zmmreg,zmmrm512|b32|sae
kreg|mask,xmmreg,xmmrm64|sae
kreg|mask,xmmreg,xmmrm32|sae
kreg|mask,xmmreg,xmmrm128|b64,imm8
kreg|mask,ymmreg,ymmrm256|b64,imm8
kreg|mask,zmmreg,zmmrm512|b64|sae,imm8
kreg|mask,xmmreg,xmmrm128|b32,imm8
kreg|mask,ymmreg,ymmrm256|b32,imm8
kreg|mask,zmmreg,zmmrm512|b32|sae,imm8
kreg|mask,xmmreg,xmmrm64|sae,imm8
kreg|mask,xmmreg,xmmrm32|sae,imm8
xmmreg,xmmrm64|sae
xmmreg,xmmrm32|sae
mem128|mask,xmmreg
mem256|mask,ymmreg
mem512|mask,zmmreg
ymmreg|mask|z,ymmreg
zmmreg|mask|z,zmmreg
xmmreg|mask|z,xmmrm64|b32
ymmreg|mask|z,xmmrm128|b32
zmmreg|mask|z,ymmrm256|b32|er
xmmreg|mask|z,xmmrm128|b32
ymmreg|mask|z,ymmrm256|b32
zmmreg|mask|z,zmmrm512|b32|er
xmmreg|mask|z,xmmrm128|b64
xmmreg|mask|z,ymmrm256|b64
ymmreg|mask|z,zmmrm512|b64|er
ymmreg|mask|z,ymmrm256|b64
zmmreg|mask|z,zmmrm512|b64|er
ymmreg|mask|z,xmmrm128
zmmreg|mask|z,ymmrm256|sae
zmmreg|mask|z,ymmrm256|b32|sae
xmmreg|mask|z,xmmreg,imm8
xmmreg|mask|z,ymmreg,imm8
ymmreg|mask|z,zmmreg|sae,imm8
mem64|mask,xmmreg,imm8
mem128|mask,ymmreg,imm8
mem256|mask,zmmreg|sae,imm8
reg32,xmmrm64|er
reg64,xmmrm64|er
xmmreg|mask|z,xmmreg,xmmrm64|er
xmmreg,xmmreg|er,rm32
xmmreg,xmmreg|er,rm64
xmmreg|mask|z,xmmreg,xmmrm32|sae
reg32,xmmrm32|er
reg64,xmmrm32|er
ymmreg|mask|z,zmmrm512|b64|sae
zmmreg|mask|z,zmmrm512|b64|sae
zmmreg|mask|z,zmmrm512|b32|sae
reg32,xmmrm64|sae
reg64,xmmrm64|sae
reg32,xmmrm32|sae
reg64,xmmrm32|sae
xmmreg|mask|z,xmmreg*,xmmrm128,imm8
ymmreg|mask|z,ymmreg*,ymmrm256,imm8
zmmreg|mask|z,zmmreg*,zmmrm512,imm8
xmmreg|mask|z,mem128
ymmreg|mask|z,mem256
zmmreg|mask|z,mem512
xmmreg|mask|z,zmmreg,imm8
mem128|mask,zmmreg,imm8
ymmreg|mask|z,zmmreg,imm8
mem256|mask,zmmreg,imm8
mem32,xmmreg,imm8
zmmreg|mask|z,zmmreg*,zmmrm512|b64|sae,imm8
zmmreg|mask|z,zmmreg*,zmmrm512|b32|sae,imm8
xmmreg|mask|z,xmmreg*,xmmrm64|sae,imm8
xmmreg|mask|z,xmmreg*,xmmrm32|sae,imm8
zmmreg|mask|z,zmmreg,zmmrm512|b64|er
zmmreg|mask|z,zmmreg,zmmrm512|b32|er
xmmreg|mask|z,xmmreg,xmmrm32|er
kreg|mask,xmmrm128|b64,imm8
kreg|mask,ymmrm256|b64,imm8
kreg|mask,zmmrm512|b64,imm8
kreg|mask,xmmrm128|b32,imm8
kreg|mask,ymmrm256|b32,imm8
kreg|mask,zmmrm512|b32,imm8
kreg|mask,xmmrm64,imm8
kreg|mask,xmmrm32,imm8
xmmreg|mask,xmem64
ymmreg|mask,xmem64
zmmreg|mask,ymem64
xmmreg|mask,xmem32
ymmreg|mask,ymem32
zmmreg|mask,zmem32
ymem64|mask
zmem32|mask
zmem64|mask
ymmreg|mask,ymem64
zmmreg|mask,zmem64
xmmreg|mask,ymem32
ymmreg|mask,zmem32
xmmreg|mask|z,xmmreg,xmmrm64|sae
xmmreg|mask|z,xmmrm128|b64,imm8
ymmreg|mask|z,ymmrm256|b64,imm8
zmmreg|mask|z,zmmrm512|b64|sae,imm8
xmmreg|mask|z,xmmrm128|b32,imm8
ymmreg|mask|z,ymmrm256|b32,imm8
zmmreg|mask|z,zmmrm512|b32|sae,imm8
xmmreg|mask|z,xmmreg,xmmrm64|sae,imm8
xmmreg|mask|z,xmmreg,xmmrm32|sae,imm8
ymmreg|mask|z,ymmreg*,xmmrm128,imm8
zmmreg|mask|z,zmmreg*,xmmrm128,imm8
zmmreg|mask|z,zmmreg*,ymmrm256,imm8
zmmreg|mask|z,zmmreg*,zmmrm512|b64|sae
zmmreg|mask|z,zmmreg*,zmmrm512|b32|sae
xmmreg|mask|z,xmmreg*,xmmrm64|sae
xmmreg|mask|z,xmmreg*,xmmrm32|sae
xmmreg|mask|z,xmmrm128
ymmreg|mask|z,ymmrm256
zmmreg|mask|z,zmmrm512
xmmrm128|mask|z,xmmreg
ymmrm256|mask|z,ymmreg
zmmrm512|mask|z,zmmreg
mem512,zmmreg
zmmreg,mem512
xmmreg|mask|z,mem64
mem64|mask,xmmreg
xmmreg|mask|z,xmmreg*,xmmreg
mem32|mask,xmmreg
zmmreg|mask|z,zmmrm512|b32
zmmreg|mask|z,zmmrm512|b64
xmmreg|mask|z,xmmreg*,xmmrm128
ymmreg|mask|z,ymmreg*,ymmrm256
zmmreg|mask|z,zmmreg*,zmmrm512
xmmreg|mask|z,xmmreg,xmmrm128
ymmreg|mask|z,ymmreg,ymmrm256
zmmreg|mask|z,zmmreg,zmmrm512
xmmreg|mask|z,xmmrm8
ymmreg|mask|z,xmmrm8
zmmreg|mask|z,xmmrm8
xmmreg|mask|z,reg8
xmmreg|mask|z,reg16
xmmreg|mask|z,reg32
xmmreg|mask|z,reg64
ymmreg|mask|z,reg8
ymmreg|mask|z,reg16
ymmreg|mask|z,reg32
ymmreg|mask|z,reg64
zmmreg|mask|z,reg8
zmmreg|mask|z,reg16
zmmreg|mask|z,reg32
zmmreg|mask|z,reg64
xmmreg,kreg
ymmreg,kreg
zmmreg,kreg
xmmreg|mask|z,xmmrm16
ymmreg|mask|z,xmmrm16
zmmreg|mask|z,xmmrm16
kreg|mask,xmmreg,xmmrm128
kreg|mask,ymmreg,ymmrm256
kreg|mask,zmmreg,zmmrm512
kreg|mask,zmmreg,zmmrm512|b32
kreg|mask,zmmreg,zmmrm512|b64
kreg|mask,xmmreg,xmmrm128,imm8
kreg|mask,ymmreg,ymmrm256,imm8
kreg|mask,zmmreg,zmmrm512,imm8
kreg|mask,zmmreg,zmmrm512|b32,imm8
kreg|mask,zmmreg,zmmrm512|b64,imm8
zmmreg|mask|z,zmmrm512|b64,imm8
zmmreg|mask|z,zmmrm512|b32,imm8
reg8,xmmreg,imm8
reg16,xmmreg,imm8
kreg,xmmreg
kreg,ymmreg
kreg,zmmreg
xmmreg|mask|z,ymmreg
xmmreg|mask|z,zmmreg
mem64|mask,ymmreg
mem128|mask,zmmreg
ymmreg|mask|z,zmmreg
mem128|mask,ymmreg
mem256|mask,zmmreg
mem16|mask,xmmreg
mem32|mask,ymmreg
mem64|mask,zmmreg
xmmreg|mask|z,xmmrm32
zmmreg|mask|z,xmmrm128
ymmreg|mask|z,xmmrm32
zmmreg|mask|z,ymmrm256
xmmreg|mask|z,xmmrm128|b32*,imm8
ymmreg|mask|z,ymmrm256|b32*,imm8
zmmreg|mask|z,zmmrm512|b32*,imm8
xmmreg|mask|z,xmmrm128|b64*,imm8
ymmreg|mask|z,ymmrm256|b64*,imm8
zmmreg|mask|z,zmmrm512|b64*,imm8
xmem32|mask,xmmreg
ymem32|mask,ymmreg
zmem32|mask,zmmreg
xmem64|mask,xmmreg
xmem64|mask,ymmreg
ymem64|mask,zmmreg
ymem32|mask,xmmreg
zmem32|mask,ymmreg
ymem64|mask,ymmreg
zmem64|mask,zmmreg
xmmreg|mask|z,xmmrm128,imm8
ymmreg|mask|z,ymmrm256,imm8
zmmreg|mask|z,zmmrm512,imm8
ymmreg|mask|z,ymmreg*,xmmrm128
zmmreg|mask|z,zmmreg*,xmmrm128
ymmreg,ymmrm256*,imm8
zmmreg,zmmrm512*,imm8
xmmreg|mask|z,xmmrm128*,imm8
ymmreg|mask|z,ymmrm256*,imm8
zmmreg|mask|z,zmmrm512*,imm8
xmmreg|mask|z,xmmreg,xmmrm128|b32,imm8
ymmreg|mask|z,ymmreg,ymmrm256|b32,imm8
zmmreg|mask|z,zmmreg,zmmrm512|b32,imm8
xmmreg|mask|z,xmmreg,xmmrm128|b64,imm8
ymmreg|mask|z,ymmreg,ymmrm256|b64,imm8
zmmreg|mask|z,zmmreg,zmmrm512|b64,imm8
xmmreg|mask|z,xmmreg*,xmmrm64
xmmreg|mask|z,xmmreg*,xmmrm32
reg_eax
reg_rax
reg32,mem512
reg64,mem512
reg32,reg_edx,reg_eax
zmmreg|mask|z,zmmreg|rs4,mem
ymmreg|mask|z,ymmreg*,ymmrm128|b32
zmmreg|mask|z,zmmreg*,zmmrm128|b32
kreg|rs2,xmmreg,xmmrm128|b32
kreg|rs2,ymmreg,ymmrm128|b32
kreg|rs2,zmmreg,zmmrm128|b32
mem512
tmmreg,tmmreg,tmmreg
tmmreg,mem
mem,tmmreg
tmmreg
xmmreg|mask|z,xmmreg*,xmmrm16|b16
ymmreg|mask|z,ymmreg*,ymmrm16|b16
zmmreg|mask|z,zmmreg*,zmmrm16|b16|er
xmmreg|mask|z,xmmreg*,xmmrm16|er
kreg|mask,xmmreg*,xmmrm16|b16,imm8
kreg|mask,ymmreg*,ymmrm16|b16,imm8
kreg|mask,zmmreg*,zmmrm16|b16|sae,imm8
kreg|mask,xmmreg*,xmmrm16|sae,imm8
xmmreg,xmmrm16|sae
xmmreg|mask|z,xmmrm64|b16
ymmreg|mask|z,xmmrm128|b16
zmmreg|mask|z,ymmrm256|b16|er
xmmreg|mask|z,xmmrm32|b16
ymmreg|mask|z,xmmrm64|b16
zmmreg|mask|z,xmmrm128|b16|sae
zmmreg|mask|z,ymmrm256|b16|sae
zmmreg|mask|z,xmmrm128|b16|er
xmmreg|mask|z,xmmrm128|b16
ymmreg|mask|z,ymmrm256|b16
zmmreg|mask|z,zmmrm512|b16|er
xmmreg,xmmreg*,xmmrm16|sae
reg32,xmmrm16|er
reg64,xmmrm16|er
xmmreg|mask|z,xmmreg*,xmmrm16|sae
xmmreg,xmmreg*,rm32|er
xmmreg,xmmreg*,rm64|er
xmmreg,xmmreg*,xmmrm32|er
zmmreg|mask|z,zmmrm512|b16|sae
reg32,xmmrm16|sae
reg64,xmmrm16|sae
xmmreg,xmmreg|er,rm32|er
xmmreg,xmmreg|er,rm64|er
xmmreg|mask|z,xmmreg*,xmmrm128|b16
ymmreg|mask|z,ymmreg*,ymmrm256|b16
zmmreg|mask|z,zmmreg*,zmmrm512|b16|er
kreg|mask,xmmrm128|b16,imm8
kreg|mask,ymmrm256|b16,imm8
kreg|mask,zmmrm512|b16,imm8
kreg|mask,xmmrm16,imm8
xmmreg|mask|z,xmmrm16|sae
xmmreg|mask|z,xmmrm128|b16,imm8
ymmreg|mask|z,ymmrm256|b16,imm8
zmmreg|mask|z,zmmrm512|b16|sae,imm8
xmmreg|mask|z,xmmrm16|sae,imm8
ymmreg|mask|z,xmmrm256|b16
xmmreg|mask|z,mem16
xmmreg|mask|z,rm16
rm16,xmmreg
zmmreg|mask|z,zmmreg*,zmmrm512|b16
xmmreg|mask|z,xmmreg*,xmmrm16|sae,imm8
mem32,reg32,reg32
mem64,reg64,reg64





mr:
hle
/r
o16
o32
o64
rm:
mi:
/2
ib,s
-i:
ib
iw
id
id,s
/0
/4
nof3
r:
ib,u
/7
/6
/5
i:
odf
rel
o64nw
m:
/3
repe
norexw
/1
ij:
r-:
-r:
wait
-:
rmi:
r+mi:
--:
a32
rel8
a64
adf
i-:
iwdq
hlexr
ri:
iq
np
norexb
f3i
m-:
mri:
mr-:
hlenl
rm-:
nohi
f2i
norep
rvm:
rvm:fv:
rvmi:
rvms:
/is4
mvr:
vmi:
rvmi:fv:
rmv:
vm32x
vm64x
vm64y
vm32y
xop.m10.lz.w0
xop.m10.lz.w1
vm:
xop.ndd.lz.m9.w0
xop.ndd.lz.m9.w1
rmx:
mxr:
mrx:
rvm:t1s:
rm:t2:
rm:t4:
rm:t8:
rm:t1s:
rvmi:t1s:
mr:t1s:
rm:hv:
rm:fv:
rm:hvm:
mri:hvm:
rm:t1f64:
rm:t1f32:
rvmi:fvm:
mri:t4:
mri:t8:
mri:t2:
mri:t1s:
rmi:fv:
rmi:t1s:
vsibx
vsiby
vsibz
m:t1s:
rvmi:t4:
rvmi:t8:
rvmi:t2:
rm:fvm:
mr:fvm:
rm:dup:
rvm:t2:
mr:t2:
rvm:fvm:
rm:t1s8:
rm:t1s16:
mri:t1s8:
mri:t1s16:
rvmi:t1s8:
rvmi:t1s16:
mr:qvm:
mr:hvm:
mr:ovm:
rm:qvm:
rm:ovm:
vmi:fv:
rmi:fvm:
rvm:m128:
vmi:fvm:
mr:t1s8:
mr:t1s16:
rvm:m128:evex.dds.512.f2.0f38.w0
rvm:m128:evex.dds.lig.f2.0f38.w0
/3r0
rm:hvm:evex.128.66.0f38.w0
rm:hvm:evex.256.66.0f38.w0
rm:hvm:evex.512.66.0f38.w0
rm:qvm:evex.128.66.map5.w0
rm:qvm:evex.256.66.map5.w0
rm:qvm:evex.512.66.map5.w0
rm:t1s:evex.lig.f3.map5.w0
rm:t1s:evex.lig.f3.map5.w1
rm:t1s:evex.128.66.map5.w0
rm:t1s:evex.256.66.map5.w0
rm:t1s:evex.512.66.map5.w0
rmvi:t1s:
mrv: